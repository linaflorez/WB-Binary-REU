{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick introduction to look for how to look at the data and what the first task will require. \n",
    "\n",
    "SDSS J114404.76+052951.77 is a white dwarf. https://ui.adsabs.harvard.edu/abs/2015ApJ...810L..17G/abstract suggested that it had a gas disc around it due to emission from Calcium. We found that the emission is actually due to an orbiting companion, probably a brown dwarf. Your task is to measure the orbital period of the system.  \n",
    "\n",
    "Some reading material on similar systems here https://ui.adsabs.harvard.edu/public-libraries/qIO0Ze9fQzGnpI49Ywtv4A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first get the python modules we need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.io.fits as fits\n",
    "import os\n",
    "import glob\n",
    "import natsort\n",
    "\n",
    "\n",
    "#matplotlib set up\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"figure.figsize\"] = (14, 5)\n",
    "rcParams[\"font.size\"] = 20\n",
    "\n",
    "#fitting the emission lines\n",
    "from astropy.modeling import models, fitting\n",
    "from astropy import constants as const\n",
    "from astropy import units as u\n",
    "import pandas as pd\n",
    "from astropy.convolution import convolve, Box1DKernel\n",
    "from scipy.signal import find_peaks\n",
    "from astropy.table import Table, Column, QTable\n",
    "from astropy.io import fits\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "#Lomb-Scargle Periodograms\n",
    "from astropy.timeseries import LombScargle as LS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fit the three emission lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting_emission_lines(w,f,num_peaks,wavelength_range, file_num):\n",
    "    ################################################################\n",
    "    #### MASKING WAVELENGTH & FLUX TO ONLY INCLUDE THE PLOTTED SECTION FROM ABOVE\n",
    "    ################################################################\n",
    "    min_range, max_range = wavelength_range[0], wavelength_range[1]\n",
    "    range_mask = (w > min_range) & (w < max_range)\n",
    "    w, f = w[range_mask], f[range_mask]\n",
    "    \n",
    "    \n",
    "    ################################################################\n",
    "    #### FITTING THE CONTINUUM USING ASTROPY, FOLLOWED BY \n",
    "    #### NORMALIZING & SMOOTHING OUT THE FLUX ACCORDINGLY\n",
    "    ################################################################\n",
    "    fitter = fitting.LinearLSQFitter()\n",
    "    n_init = models.Polynomial1D(1)\n",
    "    n_fit = fitter(n_init, w, f)\n",
    "    f = f / n_fit(w)\n",
    "    f = convolve(f, Box1DKernel(10))\n",
    "    \n",
    "    \n",
    "    ################################################################\n",
    "    #### FINDING THE PEAKS USING SCIPY\n",
    "    ################################################################\n",
    "    peaks, properties = find_peaks(f, height = 0.25, distance = 2)\n",
    "    w_peaks = w[peaks][(np.argpartition(f[peaks], -num_peaks)[-num_peaks:])]\n",
    "    f_peaks = f[peaks][(np.argpartition(f[peaks], -num_peaks)[-num_peaks:])]\n",
    "\n",
    "\n",
    "    ################################################################\n",
    "    #### FITTING THE PEAKS USING ASTROPY\n",
    "    ################################################################\n",
    "    for i in range(num_peaks):\n",
    "        if i == 0:\n",
    "            model = models.Const1D(1.0) + models.Gaussian1D(1.0, w_peaks[i], 1)\n",
    "        else:   \n",
    "            model = model + models.Gaussian1D(1.0, w_peaks[i], 1)\n",
    "        \n",
    "        \n",
    "    ################################################################\n",
    "    #### MAKING THE FITS\n",
    "    ################################################################\n",
    "    fitter = fitting.LevMarLSQFitter()\n",
    "    g_peak_norm = fitter(model, w, f)\n",
    "    fit_info = fitter.fit_info[\"param_cov\"]\n",
    "    \n",
    "    return w,f,w[peaks],f[peaks],n_fit, g_peak_norm, fit_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make a csv of all the neccesary compiled information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_table(NIR_wavelengths,csv_path_files, fits_path_files, num_peaks,wavelength_range):\n",
    "#     table_name = input(\"File name for the table?\")\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #### ITERATING THROUGH ALL THE CSV & FITS FILES\n",
    "    ################################################################\n",
    "    sp = csv_path_files\n",
    "    sp_f = fits_path_files\n",
    "\n",
    "    \n",
    "    ###############################################################\n",
    "    #### STORING THE INFO FOR TABLE\n",
    "    ################################################################    \n",
    "    tracking_file_num = np.linspace(0,len(csv_path_files),len(csv_path_files))\n",
    "    storing_velocities = np.zeros((len(csv_path_files),num_peaks))\n",
    "    fit_error = np.zeros((len(csv_path_files),num_peaks))\n",
    "    MJD = np.zeros(np.shape(sp)[0])\n",
    "    file_path = []\n",
    "    detections = []\n",
    "    fit_peaks = np.zeros((len(csv_path_files),num_peaks))\n",
    "\n",
    "    ################################################################\n",
    "    #### GETTING WAVELENGTHS OF MAIN PEAKS, MJDS, & CALCULATING DOPPLER VELOCITIES\n",
    "    ################################################################\n",
    "    for i in range(0,np.shape(sp)[0]):\n",
    "        ###############################################################\n",
    "        #### GRABBING INFO FROM CSV FILES & FIT FUNCTION\n",
    "        ################################################################ \n",
    "        s = sp[i]\n",
    "        file_path.append(s.split(vispath,1)[1])\n",
    "        w, f, e = np.loadtxt(s, unpack=True, delimiter=',')\n",
    "        mod_w,mod_f,w_of_peaks,f_of_peaks, n_fit, main_peak_values,fit_e = fitting_emission_lines(w,f,num_peaks, wavelength_range,i)\n",
    "        \n",
    "\n",
    "        ###############################################################\n",
    "        #### GRABBING TIME FROM CORRESPONDING FITS FILES\n",
    "        ################################################################ \n",
    "        s_fits = sp_f[i]\n",
    "        hdulist = fits.open(s_fits)\n",
    "        hdu = hdulist[0]\n",
    "        MJD_OBS, EXPTIME = hdu.header[\"MJD-OBS\"], hdu.header[\"EXPTIME\"]\n",
    "        time = ((MJD_OBS * u.d) + (((EXPTIME/2)*u.s).to(u.d))).value\n",
    "        MJD[i] = time\n",
    "\n",
    "        \n",
    "        ################################################################\n",
    "        #### DETERMINING ERROR\n",
    "        ################################################################\n",
    "        if fit_e is not None:\n",
    "            fit_error[i] = (np.sqrt(np.diag(fit_e))[2:-1:3])**2 #variance\n",
    "        if fit_e is None:\n",
    "            fit_error[i] = np.zeros(num_peaks)\n",
    "        \n",
    "        \n",
    "        ################################################################\n",
    "        #### EQUATION FOR DOPPLER VELOCITY:\n",
    "        #### SPEED OF LIGHT * ((EMITTED - OBSERVED) / EMITTED)) = VELOCITY\n",
    "        ################################################################\n",
    "        peak_w = np.sort(main_peak_values.parameters[2:-1:3])\n",
    "        calculated_shift = np.zeros(num_peaks)\n",
    "        c = ((const.c).to(u.km/u.s)).value\n",
    "        for j in range(num_peaks):\n",
    "            shifted_peak = (peak_w[j] - NIR_wavelengths[j])\n",
    "            doppler_velocities = c * (shifted_peak / NIR_wavelengths[j])\n",
    "            storing_velocities[i][j] = doppler_velocities\n",
    "            fit_peaks[i][j] = peak_w[j]\n",
    "            calculated_shift[j] = shifted_peak\n",
    "\n",
    "        ################################################################\n",
    "        #### DETERMINING DETECTION\n",
    "        ################################################################\n",
    "        sign = np.sign(calculated_shift)\n",
    "        if ((all(elements == 1 for elements in sign)) or (all(elements == -1 for elements in sign))) and ((all(abs(values) <= 30 for values in calculated_shift))):\n",
    "            detections.append(\"Yes\")\n",
    "        else:\n",
    "            detections.append(\"No\")\n",
    "    \n",
    "    ###############################################################\n",
    "    #### MAKING A DATAFRAME THAT CONTAINS THE MJD, DETECTIONS, & FILE PATH\n",
    "    ################################################################    \n",
    "    basic_info = pd.DataFrame()\n",
    "    basic_info[\"MJD\"] = MJD\n",
    "    basic_info[\"Detections\"] = detections\n",
    "    basic_info[\"File Path\"] = file_path\n",
    "\n",
    "    ###############################################################\n",
    "    #### MAKING A DATAFRAME TO INCLUDE DV & WEIGHTED MEAN/ERROR\n",
    "    ################################################################ \n",
    "    fit_df = pd.DataFrame()\n",
    "    for k in range(num_peaks):\n",
    "        fit_df['DV%i' % (k+1)] = storing_velocities[:,k]\n",
    "        fit_df['F%i' % (k+1)] = fit_peaks[:,k]\n",
    "        fit_df['FE%i' % (k+1)] = fit_error[:,k]\n",
    "\n",
    "        \n",
    "    ###############################################################\n",
    "    #### DETERMINING WEIGHTED ERROR FOR EACH DV CALCULATION\n",
    "    ################################################################ \n",
    "    for l in range(num_peaks):\n",
    "        maxima = fit_df['F%i' % (l+1)] - fit_df['FE%i' % (l+1)]\n",
    "        max_dv = c * (NIR_wavelengths[l] - maxima)/ NIR_wavelengths[l]\n",
    "        max_dv_off = max_dv - fit_df['DV%i' % (l+1)]\n",
    "\n",
    "        minima = fit_df['F%i' % (l+1)] + fit_df['FE%i' % (l+1)]\n",
    "        min_dv = c* (NIR_wavelengths[l] - minima)/ NIR_wavelengths[l]\n",
    "        min_dv_off = min_dv - fit_df['DV%i' % (l+1)] \n",
    "        \n",
    "        fit_df[\"ERROR%i\" % (l+1)] = (abs(max_dv_off) + abs(min_dv_off)/2)\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #### MAKING WEIGHTED MEAN\n",
    "    ################################################################ \n",
    "    for l in range(num_peaks):\n",
    "        value1, value2 = fit_df['F%i' % (l+1)] + fit_df['FE%i' % (l+1)], fit_df['F%i' % (l+1)] - fit_df['FE%i' % (l+1)]\n",
    "        fit_df[\"blah%i\" % (l+1)] = ((value1 + value2)/2)**-2\n",
    "        \n",
    "    weighted_mean_num = (fit_df[\"DV1\"] * fit_df[\"blah1\"]) + (fit_df[\"DV2\"] * fit_df[\"blah2\"]) + (fit_df[\"DV3\"] * fit_df[\"blah3\"])\n",
    "    weighted_err_num = (fit_df[\"ERROR1\"] * fit_df[\"blah1\"]) + (fit_df[\"ERROR2\"] * fit_df[\"blah2\"]) + (fit_df[\"ERROR3\"] * fit_df[\"blah3\"])\n",
    "    weighted_mean_denom = fit_df[\"blah1\"] + fit_df[\"blah2\"] + fit_df[\"blah3\"]\n",
    "    \n",
    "    fit_df[\"Weighted Mean\"] = weighted_mean_num/weighted_mean_denom\n",
    "    fit_df[\"Weighted Error\"] = (weighted_err_num)/weighted_mean_denom\n",
    "\n",
    "    \n",
    "    fit_df = fit_df.drop(columns=[\"blah1\",\"blah2\",\"blah3\"])\n",
    "    fit_df = fit_df.drop(columns=[\"F1\",\"F2\",\"F3\",\"FE1\",\"FE2\",\"FE3\" ])  \n",
    "    fit_df = fit_df.drop(columns=[\"ERROR1\",\"ERROR2\",\"ERROR3\"])\n",
    "    fit_df = fit_df.reindex_axis(['DV1','DV2','DV3',\"Weighted Mean\",\"Weighted Error\"], axis=1)\n",
    "    fit_df = fit_df.rename({'DV1': 'DV1 (km/s)', 'DV2': 'DV2 (km/s)', 'DV3': 'DV3 (km/s)'}, axis=1) \n",
    "    \n",
    "    table = pd.concat([fit_df, basic_info], axis=1, sort=False)\n",
    "    \n",
    "    ###############################################################\n",
    "    #### COMBINING BOTH DATAFRAMES TO FORM THE CSV TABLE\n",
    "    ################################################################\n",
    "#     t = Table.from_pandas(table)\n",
    "#     making_table = t.write(\"%s.csv\" % (table_name), format=\"ascii.csv\", overwrite=True)\n",
    "    \n",
    "    return storing_velocities, MJD, detections, fit_df[\"Weighted Mean\"], fit_df[\"Weighted Error\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to plot the fits & check they were accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_fit_and_calcs(NIR_wavelengths,csv_path_files, fits_path_files, num_peaks,wavelength_range):\n",
    "#     plot_name = input(\"File name for the plots?\")\n",
    "\n",
    "    \n",
    "    ###############################################################\n",
    "    #### GRABBING INFO FROM PREVIOUS FUNCTION\n",
    "    ################################################################ \n",
    "    storing_velocities, MJD, detection, weighted_mean, weighted_error = making_table(NIR_wavelengths,csv_path_files, fits_path_files, num_peaks,wavelength_range)\n",
    "    \n",
    "    \n",
    "    ################################################################\n",
    "    #### PLOTTING TO CHECK DETECTION & FIT\n",
    "    ################################################################\n",
    "\n",
    "    sp = csv_path_files\n",
    "    for i in range(0,np.shape(sp)[0]):\n",
    "        ###############################################################\n",
    "        #### GRABBING INFO FROM CSV FILES & FIT FUNCTION\n",
    "        ################################################################ \n",
    "        s = sp[i]\n",
    "        w, f, e = np.loadtxt(s, unpack=True, delimiter=',')\n",
    "        w,f,w_peak,f_peak, n_fit, fit,fit_e = fitting_emission_lines(w,f,num_peaks, wavelength_range,i)\n",
    "        \n",
    "        plt.style.use(\"seaborn-ticks\")\n",
    "        plt.figure(num=None, figsize=(18, 6),facecolor='w', edgecolor='k')\n",
    "#         plt.title(\"From File Number: %i \\n Detection: %s\" % (i, detection[i]), weight = \"bold\", size = 20)\n",
    "        plt.plot(w, f, \"m\", markersize = 10, label = \"Normalized Data\")\n",
    "\n",
    "\n",
    "        ################################################################\n",
    "        #### PLOTTING FITS DATA\n",
    "        ################################################################\n",
    "        plt.axvline(NIR_wavelengths[0], color = \"white\", label = \"NIR ${\\lambda}$ values\")\n",
    "        plt.axvline(NIR_wavelengths[1], color = \"white\")\n",
    "        plt.axvline(NIR_wavelengths[2], color = \"white\")\n",
    "\n",
    "#         plt.plot(w_peak[(np.argpartition(f_peak, -num_peaks)[-num_peaks:])],f_peak[(np.argpartition(f_peak, -num_peaks)[-num_peaks:])],\"bo\", label = \"scipy (each peak)\")\n",
    "        plt.plot(w, fit(w),\"k--\", lw = 3, label = \"Gaussian Fit\")\n",
    "#         plt.plot(w, n_fit(w)+1, \"m.\", label = \"Continuum Fit\")\n",
    "        plt.xlabel(\"Wavelength (${\\AA}$)\", weight = \"bold\", size = 15)\n",
    "        plt.ylabel(\"Normalized Flux\", weight = \"bold\", size = 15)\n",
    "        plt.legend(loc = \"best\", prop = {\"size\" : 15})\n",
    "        plt.tick_params(axis= \"both\", which = \"major\", labelsize = 13)\n",
    "        plt.xlim(wavelength_range[0],wavelength_range[1])\n",
    "        plt.xlim(8480,8680)\n",
    "        plt.ylim(0.8,1.7)\n",
    "        plt.savefig(\"/Users/linaflorez/Desktop/UT_TAURUS/Plots/Calcium_Fits/%i.pdf\" % (i), transparent = True)\n",
    "#         plt.close()\n",
    "        plt.show()\n",
    "\n",
    "    ################################################################\n",
    "    #### PLOTTING TO DETERMINE IF THERE'S TIME DEPENDENCE\n",
    "    ################################################################  \n",
    "    index = []\n",
    "    average_vel = np.zeros(len(detection))\n",
    "    for i in range(0,len(detection)):\n",
    "        if detection[i] == \"Yes\":\n",
    "            index.append(i)\n",
    "        \n",
    "        average_vel[i] = np.mean(storing_velocities[i])\n",
    "\n",
    "    time_x = (np.asarray(MJD)[index] * u.d).to(u.hour).value\n",
    "    velocity_y = (average_vel[index])\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1,2,figsize=(20,6))\n",
    "    ax1.set_title(\"Time v Doppler Velocity \\n %s\" % (plot_name), weight = \"bold\", size = 15)\n",
    "    ax1.plot(time_x, velocity_y, '.', color = \"blue\", markersize = 10)\n",
    "    ax1.set_xlabel(\"Time (hours)\", weight = \"bold\", size = 15)\n",
    "    ax1.set_ylabel(\"Velocity (km/s)\", weight = \"bold\", size = 15)\n",
    "    ax1.tick_params(\"both\", labelsize = 15)\n",
    "    ax1.set_xlim(1406400 + 25,1406400 + 30)\n",
    "\n",
    "    ax2.set_title(\"Time v Doppler Velocity\", weight = \"bold\", size = 15)\n",
    "    ax2.plot(time_x, velocity_y,  '.', color = \"blue\", markersize = 10)\n",
    "    ax2.set_xlabel(\"Time (hours)\", weight = \"bold\", size = 15)\n",
    "    ax2.set_ylabel(\"Velocity (km/s)\", weight = \"bold\", size = 15)\n",
    "    ax2.tick_params(\"both\", labelsize = 15)\n",
    "    ax2.set_xlim(1406400 + 48,1406400 + 55)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.25, hspace=0)\n",
    "    plt.savefig(\"/Users/linaflorez/Desktop/UT_TAURUS/Plots/Calcium_Fits/%i.pdf\" % (i), transparent = true)\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    \n",
    "    return w_peak, f_peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2019 Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### ORIGINAL DATA\n",
    "################################################################\n",
    "path = \"/Users/linaflorez/Desktop/UT_TAURUS/WDJ114404.76+052951.77/\" #note the / on the end\n",
    "#print(os.listdir(path))\n",
    "vispath = path + 'VIS_notell/'\n",
    "sp_csv = natsort.natsorted(glob.glob(vispath+'*TAC.csv')) #makes an array containing the path to all csv files. Note the TAC as I didn't want to include the merged spectrum\n",
    "sp_fits = natsort.natsorted(glob.glob(vispath+'*TAC.fits')) #makes an array containing the path to all csv files. Note the TAC as I didn't want to include the merged spectrum\n",
    "peak_wavelength_range = [(8494,8498),(8538,8542.5),(8657.75,8662.5)]\n",
    "wavelength_range = (8450,8700)\n",
    "NIR_wavelengths = [8498.02, 8542.09, 8662.14]\n",
    "num_peaks = 3\n",
    "ws,fs = checking_fit_and_calcs(NIR_wavelengths,sp_csv,sp_fits,num_peaks, wavelength_range)\n",
    "#plots: OriginalData/\n",
    "#table: CSV_FILES/OriginalData\n",
    "\n",
    "# making_table(NIR_wavelengths,sp_csv, sp_fits, num_peaks,wavelength_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for time dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn\")\n",
    "df = pd.read_csv(\"CSV_FILES/OriginalData.csv\")\n",
    "data = df.loc[df['Detections'] == 'Yes']\n",
    "masked = data.loc[(data['MJD'] > (2.05 + 5.86e4))]\n",
    "TIME = ((masked[\"MJD\"]).values * u.d).to(u.hour)\n",
    "\n",
    "plt.plot(TIME, masked[\"Weighted Mean\"], \"k.\", markersize = 15)\n",
    "plt.xlabel(\"Time (Hour)\", weight = \"bold\", size = 15)\n",
    "plt.xlim(49 + 1.4064e6)\n",
    "plt.ylabel(\"Velocity (km/s)\", weight = \"bold\", size = 15)\n",
    "plt.tick_params(\"both\", labelsize = 15)\n",
    "plt.show()\n",
    "print(np.max(TIME) - np.min(TIME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#### NEW/OLD DATA\n",
    "################################################################\n",
    "path = \"/Users/linaflorez/Desktop/Summer2019/\" #note the / on the end\n",
    "#print(os.listdir(path))\n",
    "vispath = path + 'SDSSJ1144_old2/'\n",
    "sp_csv = natsort.natsorted(glob.glob(vispath+'*TAC.csv')) #makes an array containing the path to all csv files. Note the TAC as I didn't want to include the merged spectrum\n",
    "sp_fits = natsort.natsorted(glob.glob(vispath+'*TAC.fits')) #makes an array containing the path to all csv files. Note the TAC as I didn't want to include the merged spectrum\n",
    "wavelength_range = (8450,8700)\n",
    "NIR_wavelengths = [8498.02, 8542.09, 8662.14]\n",
    "num_peaks = 3\n",
    "#plots: NEW_OLD/\n",
    "#table: CSV_FILES/NEW_OLD\n",
    "\n",
    "making_table(NIR_wavelengths,sp_csv, sp_fits, num_peaks,wavelength_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
